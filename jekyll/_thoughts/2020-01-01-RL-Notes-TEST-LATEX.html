---
layout: default
title: RL and LaTeX
math: true
---

<p>
&#x2026;
</p>

<div id="outline-container-org825d785" class="outline-2">
<h2 id="org825d785">RL drill</h2>
<div class="outline-text-2" id="text-org825d785">
</div>
<div id="outline-container-org2e9ee6e" class="outline-3">
<h3 id="org2e9ee6e">MDP Bellman Equation for Expected Rewards&#xa0;&#xa0;&#xa0;<span class="tag"><span class="drill">drill</span></span></h3>
<div class="outline-text-3" id="text-org2e9ee6e">
<ul class="org-ul">
<li>This equation describes the expected reward for taking the action prescribed by some policy \(\pi\):</li>
<li>\(V^{\pi}(s) :=\) [\(R(s,\pi(s)) + \gamma \Sigma_{s'} P(s'|s, \pi(s)) V^{\pi}(s')\)]</li>
</ul>
</div>
</div>
<div id="outline-container-orgb3aca7a" class="outline-3">
<h3 id="orgb3aca7a">MDP Bellman Optimality Equation&#xa0;&#xa0;&#xa0;<span class="tag"><span class="drill">drill</span></span></h3>
<div class="outline-text-3" id="text-orgb3aca7a">
<ul class="org-ul">
<li>This equation describes the reward for taking the action giving the highest expected return. Where \(\pi*\) is the optimal policy and \(V^{\pi*}\) refers to the value function of the optimal policy:</li>
<li>\(V^{\pi*}(s) :=\) [\(\underset{a}{\max} {R(s,a) + \gamma \Sigma_{s'} P(s'|s, a) V^{\pi*}(s')}\)]</li>
</ul>
</div>
</div>
</div>
